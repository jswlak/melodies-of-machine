# ğŸ§  Neural Network Basics

Welcome to the **Neural Network Basics** section of the `melodies_of_machine` deep learning repository. This folder is dedicated to building a solid foundational understanding of how neural networks work â€” from the structure of a single neuron to full training cycles with optimizers and regularization.

ğŸ“ **Path**: `melodies_of_machine/03_deep_learning/00_neural_nets_basics/`

---

## ğŸ—‚ï¸ Contents

| No. | Notebook | Description | Status |
|-----|----------|-------------|--------|
| 01 | `01_neuron_basics.ipynb` | Understand the basic computational unit â€” the artificial neuron. | âœ… |
| 02 | `02_multilayer_perceptron.ipynb` | Learn how neurons are stacked to form a multilayer perceptron (MLP). | âœ… |
| 03 | `03_backprop_from_scratch.ipynb` | Implement the backpropagation algorithm from scratch (no frameworks). | âœ… |
| 04 | `04_activation_functions.ipynb` | Explore activation functions like ReLU, Sigmoid, Tanh and their roles. | âœ… |
| 05 | `05_loss_functions.ipynb` | Dive into different loss functions and their gradients. | âœ… |
| 06 | `06_weight_initialization.ipynb` | Learn how initial weights affect training, and explore common techniques. | âœ… |
| 07 | `07_optimizers.ipynb` | Understand optimizers like SGD, Momentum, RMSprop, and Adam. | âœ… |
| 08 | `08_learning_rate_and_schedulers.ipynb` | Explore how learning rates work and different scheduling strategies. | ğŸ”œ |
| 09 | `09_regularization_techniques.ipynb` | Implement L1, L2, and dropout to reduce overfitting. | ğŸ”œ |
| 10 | `10_gradient_problems.ipynb` | Understand vanishing/exploding gradients and how to address them. | ğŸ”œ |
| 11 | `11_mini_batch_vs_batch_vs_sgd.ipynb` | Compare different training strategies: batch, mini-batch, and SGD. | ğŸ”œ |
| 12 | `12_forward_backward_pass.ipynb` | Full forward and backward pass through a network. | ğŸ”œ |

---

## ğŸ§­ Goal

By the end of this module, you should be able to:
- Understand how neural networks learn
- Code MLPs from scratch
- Apply common training strategies and tricks
- Build intuition for why and how deep learning works

---

## âœ… Prerequisites

- Python basics
- NumPy
- High school math (linear algebra, calculus)
- Curiosity to learn deeply

---




