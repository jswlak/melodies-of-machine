{"cells": [{"cell_type": "code", "metadata": {}, "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\ntrain_ds = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n\nclass SmallCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(1,16,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(16,32,3,padding=1), nn.ReLU(), nn.MaxPool2d(2),\n        )\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(32*7*7,128), nn.ReLU(),\n            nn.Linear(128,10)\n        )\n    def forward(self,x):\n        return self.fc(self.conv(x))\n\nmodel = SmallCNN().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\nfor batch,(xb,yb) in enumerate(train_loader):\n    xb, yb = xb.to(device), yb.to(device)\n    logits = model(xb)\n    loss = criterion(logits, yb)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    if batch%200==0:\n        print(batch, loss.item())\n\nprint(\"Done.\")", "outputs": [], "execution_count": null}], "metadata": {}, "nbformat": 4, "nbformat_minor": 5}